{
  "benchmark_id": "vqav2-(test)",
  "name": "VQAv2 (test)",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "reasoning"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "VQA v2.0 (Visual Question Answering v2.0) is a balanced dataset designed to counter language priors in visual question answering. It consists of complementary image pairs where the same question yields different answers, forcing models to rely on visual understanding rather than language bias. The dataset contains 1,105,904 questions across 204,721 COCO images, requiring understanding of vision, language, and commonsense knowledge.",
  "paper_link": "https://arxiv.org/abs/1612.00837",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.430940+00:00",
  "updated_at": "2025-07-19T19:56:14.430940+00:00"
}