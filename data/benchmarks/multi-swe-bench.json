{
  "benchmark_id": "multi-swe-bench",
  "name": "Multi-SWE-Bench",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "code"],
  "modality": "text",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "A multilingual benchmark for issue resolving that evaluates Large Language Models' ability to resolve software issues across diverse programming ecosystems. Covers 7 programming languages (Java, TypeScript, JavaScript, Go, Rust, C, and C++) with 1,632 high-quality instances carefully annotated by 68 expert annotators. Addresses limitations of existing benchmarks that focus almost exclusively on Python.",
  "paper_link": "https://arxiv.org/abs/2504.02605",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-09-15T00:00:00.000000+00:00",
  "updated_at": "2025-09-15T00:00:00.000000+00:00"
}
