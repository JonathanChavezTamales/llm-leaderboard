{
  "benchmark_id": "multipl-e-humaneval",
  "name": "Multipl-E HumanEval",
  "parent_benchmark_id": null,
  "categories": ["language", "general"],
  "modality": "text",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "MultiPL-E is a scalable and extensible approach to benchmarking neural code generation that translates unit test-driven code generation benchmarks across multiple programming languages. It extends the HumanEval benchmark to 18 additional programming languages, enabling evaluation of code generation models across diverse programming paradigms and providing insights into how models generalize programming knowledge across language boundaries.",
  "paper_link": "https://arxiv.org/abs/2208.08227",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.345081+00:00",
  "updated_at": "2025-07-19T19:56:14.345081+00:00"
}