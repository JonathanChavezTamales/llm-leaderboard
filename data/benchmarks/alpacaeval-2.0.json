{
  "benchmark_id": "alpacaeval-2.0",
  "name": "AlpacaEval 2.0",
  "parent_benchmark_id": null,
  "categories": ["general", "creativity", "reasoning"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",
  "paper_link": "https://arxiv.org/abs/2404.04475",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:15.038178+00:00",
  "updated_at": "2025-07-19T19:56:15.038178+00:00"
}