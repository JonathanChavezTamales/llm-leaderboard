{
  "benchmark_id": "zebralogic",
  "name": "ZebraLogic",
  "parent_benchmark_id": null,
  "categories": ["reasoning"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "ZebraLogic is an evaluation framework for assessing large language models' logical reasoning capabilities through logic grid puzzles derived from constraint satisfaction problems (CSPs). The benchmark consists of 1,000 programmatically generated puzzles with controllable and quantifiable complexity, revealing a 'curse of complexity' where model accuracy declines significantly as problem complexity grows.",
  "paper_link": "https://arxiv.org/abs/2502.01100",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-09-05T00:00:00.000000+00:00",
  "updated_at": "2025-09-05T00:00:00.000000+00:00"
}
