{
  "benchmark_id": "mmmu-pro",
  "name": "MMMU-Pro",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "reasoning", "general"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",
  "paper_link": "https://arxiv.org/abs/2409.02813",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.282252+00:00",
  "updated_at": "2025-07-19T19:56:14.282252+00:00"
}