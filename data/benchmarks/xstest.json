{
  "benchmark_id": "xstest",
  "name": "XSTest",
  "parent_benchmark_id": null,
  "categories": ["safety"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "XSTest is a test suite designed to identify exaggerated safety behaviours in large language models. It comprises 450 prompts: 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models should refuse. The benchmark systematically evaluates whether models refuse to respond to clearly safe prompts due to overly cautious safety mechanisms.",
  "paper_link": "https://arxiv.org/abs/2308.01263",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:13.998594+00:00",
  "updated_at": "2025-07-19T19:56:13.998594+00:00"
}