{
  "benchmark_id": "triviaqa",
  "name": "TriviaQA",
  "parent_benchmark_id": null,
  "categories": ["general", "reasoning"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",
  "paper_link": "https://arxiv.org/abs/1705.03551",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:11.563587+00:00",
  "updated_at": "2025-07-19T19:56:11.563587+00:00"
}