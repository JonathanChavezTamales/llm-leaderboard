{
  "benchmark_id": "arena-hard-v2",
  "name": "Arena-Hard v2",
  "parent_benchmark_id": null,
  "categories": ["general", "reasoning", "creativity"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",
  "paper_link": "https://arxiv.org/abs/2406.11939",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-08-03T22:06:11.411643+00:00",
  "updated_at": "2025-08-03T22:06:11.411643+00:00"
}