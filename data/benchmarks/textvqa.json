{
  "benchmark_id": "textvqa",
  "name": "TextVQA",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "image-to-text"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",
  "paper_link": "https://arxiv.org/abs/1904.08920",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:12.875287+00:00",
  "updated_at": "2025-07-19T19:56:12.875287+00:00"
}