{
  "benchmark_id": "arc-c",
  "name": "ARC-C",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "general"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",
  "paper_link": "https://arxiv.org/abs/1803.05457",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:11.052939+00:00",
  "updated_at": "2025-07-19T19:56:11.052939+00:00"
}