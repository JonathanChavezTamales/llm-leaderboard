{
  "benchmark_id": "writingbench",
  "name": "WritingBench",
  "parent_benchmark_id": null,
  "categories": ["writing", "creativity", "communication"],
  "modality": "text",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",
  "paper_link": "https://arxiv.org/abs/2503.05244",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-08-03T22:06:11.074130+00:00",
  "updated_at": "2025-08-03T22:06:11.074130+00:00"
}