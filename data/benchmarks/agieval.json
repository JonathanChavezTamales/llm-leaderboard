{
  "benchmark_id": "agieval",
  "name": "AGIEval",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "general", "math"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",
  "paper_link": "https://arxiv.org/abs/2304.06364",
  "implementation_link": "https://github.com/ruixiangcui/AGIEval",
  "verified": false,
  "created_at": "2025-07-19T19:56:13.970928+00:00",
  "updated_at": "2025-07-19T19:56:13.970928+00:00"
}
