{
  "benchmark_id": "cc-ocr",
  "name": "CC-OCR",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "text-to-image"],
  "modality": "multimodal",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "A comprehensive OCR benchmark for evaluating Large Multimodal Models (LMMs) in literacy. Comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. Contains 39 subsets with 7,058 fully annotated images, 41% sourced from real applications. Tests capabilities including text grounding, multi-orientation text recognition, and detecting hallucination/repetition across diverse visual challenges.",
  "paper_link": "https://arxiv.org/abs/2412.02210",
  "implementation_link": "https://github.com/AlibabaResearch/AdvancedLiterateMachinery",
  "verified": false,
  "created_at": "2025-07-19T19:56:14.652986+00:00",
  "updated_at": "2025-07-19T19:56:14.652986+00:00"
}