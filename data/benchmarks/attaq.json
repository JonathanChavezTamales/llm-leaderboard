{
  "benchmark_id": "attaq",
  "name": "AttaQ",
  "parent_benchmark_id": null,
  "categories": ["safety"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "AttaQ is a unique dataset containing adversarial examples in the form of questions designed to provoke harmful or inappropriate responses from large language models. The benchmark evaluates safety vulnerabilities by using specialized clustering techniques that analyze both the semantic similarity of input attacks and the harmfulness of model responses, facilitating targeted improvements to model safety mechanisms.",
  "paper_link": "https://arxiv.org/abs/2311.04124",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:15.079764+00:00",
  "updated_at": "2025-07-19T19:56:15.079764+00:00"
}