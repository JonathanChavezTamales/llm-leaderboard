{
  "benchmark_id": "crux-o",
  "name": "CRUX-O",
  "parent_benchmark_id": null,
  "categories": ["reasoning"],
  "modality": "text",
  "multilingual": false,
  "max_score": 100.0,
  "language": "en",
  "description": "CRUXEval-O (output prediction) is part of the CRUXEval benchmark consisting of 800 Python functions (3-13 lines) designed to evaluate AI models' capabilities in code reasoning, understanding, and execution. The benchmark tests models' ability to predict correct function outputs given function code and inputs, focusing on short problems that a good human programmer should be able to solve in a minute.",
  "paper_link": "https://arxiv.org/abs/2401.03065",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.635245+00:00",
  "updated_at": "2025-07-19T19:56:14.635245+00:00"
}