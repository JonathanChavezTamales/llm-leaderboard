{
  "benchmark_id": "truthfulqa",
  "name": "TruthfulQA",
  "parent_benchmark_id": null,
  "categories": ["general", "reasoning", "legal", "healthcare", "finance"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",
  "paper_link": "https://arxiv.org/abs/2109.07958",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:11.339268+00:00",
  "updated_at": "2025-07-19T19:56:11.339268+00:00"
}