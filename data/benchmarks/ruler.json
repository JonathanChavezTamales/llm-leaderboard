{
  "benchmark_id": "ruler",
  "name": "RULER",
  "parent_benchmark_id": null,
  "categories": ["long_context", "reasoning"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "RULER (What's the Real Context Size of Your Long-Context Language Models?) is a synthetic benchmark designed to comprehensively evaluate the long-context capabilities of language models. It expands on needle-in-a-haystack (NIAH) testing by introducing new task categories including multi-hop tracing and aggregation tasks. The benchmark provides flexible configurations for customized sequence length and task complexity, evaluating 17 long-context language models across 13 representative tasks to reveal that despite models claiming 32K+ token context sizes, only half maintain satisfactory performance at 32K length.",
  "paper_link": "https://arxiv.org/abs/2404.06654",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.175181+00:00",
  "updated_at": "2025-07-19T19:56:14.175181+00:00"
}