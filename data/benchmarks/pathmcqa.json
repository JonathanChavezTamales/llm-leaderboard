{
  "benchmark_id": "pathmcqa",
  "name": "PathMCQA",
  "parent_benchmark_id": null,
  "categories": ["healthcare", "vision", "multimodal", "reasoning"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "PathMMU is a massive multimodal expert-level benchmark for understanding and reasoning in pathology, containing 33,428 multimodal multi-choice questions and 24,067 images validated by seven pathologists. It evaluates Large Multimodal Models (LMMs) performance on pathology tasks, with the top-performing model GPT-4V achieving only 49.8% zero-shot performance compared to 71.8% for human pathologists.",
  "paper_link": "https://arxiv.org/abs/2401.16355",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.036453+00:00",
  "updated_at": "2025-07-19T19:56:14.036453+00:00"
}