{
  "benchmark_id": "arena-hard",
  "name": "Arena Hard",
  "parent_benchmark_id": null,
  "categories": ["general", "reasoning", "creativity"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",
  "paper_link": "https://arxiv.org/abs/2406.11939",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.079874+00:00",
  "updated_at": "2025-07-19T19:56:14.079874+00:00"
}