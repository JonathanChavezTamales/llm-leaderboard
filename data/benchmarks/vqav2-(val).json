{
  "benchmark_id": "vqav2-(val)",
  "name": "VQAv2 (val)",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "language", "reasoning"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "VQAv2 is a balanced Visual Question Answering dataset containing open-ended questions about images that require understanding of vision, language, and commonsense knowledge to answer. VQAv2 addresses bias issues from the original VQA dataset by collecting complementary images such that every question is associated with similar images that result in different answers, forcing models to actually understand visual content rather than relying on language priors.",
  "paper_link": "https://arxiv.org/abs/1612.00837",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:13.647852+00:00",
  "updated_at": "2025-07-19T19:56:13.647852+00:00"
}