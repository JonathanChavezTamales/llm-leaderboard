{
  "benchmark_id": "swe-lancer",
  "name": "SWE-Lancer",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "code"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "A benchmark for evaluating large language models on real-world freelance software engineering tasks from Upwork. Contains over 1,400 tasks valued at $1 million USD total, ranging from $50 bug fixes to $32,000 feature implementations. Includes both independent engineering tasks graded via end-to-end tests and managerial tasks assessed against original engineering managers' choices.",
  "paper_link": "https://arxiv.org/abs/2502.12115",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:15.352660+00:00",
  "updated_at": "2025-07-19T19:56:15.352660+00:00"
}
