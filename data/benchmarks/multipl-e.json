{
  "benchmark_id": "multipl-e",
  "name": "MultiPL-E",
  "parent_benchmark_id": null,
  "categories": ["general", "language"],
  "modality": "text",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",
  "paper_link": "https://arxiv.org/abs/2208.08227",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:12.311919+00:00",
  "updated_at": "2025-07-19T19:56:12.311919+00:00"
}
