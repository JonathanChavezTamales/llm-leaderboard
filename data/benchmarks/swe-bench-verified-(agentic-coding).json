{
  "benchmark_id": "swe-bench-verified-(agentic-coding)",
  "name": "SWE-bench Verified (Agentic Coding)",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "code"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "SWE-bench Verified is a human-filtered subset of 500 software engineering problems drawn from real GitHub issues across 12 popular Python repositories. Given a codebase and an issue description, language models are tasked with generating patches that resolve the described problems. This benchmark evaluates AI's real-world agentic coding skills by requiring models to navigate complex codebases, understand software engineering problems, and coordinate changes across multiple functions, classes, and files to fix well-defined issues with clear descriptions.",
  "paper_link": "https://arxiv.org/abs/2310.06770",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:12.331440+00:00",
  "updated_at": "2025-07-19T19:56:12.331440+00:00"
}
