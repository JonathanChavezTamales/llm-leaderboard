{
  "benchmark_id": "mt-bench",
  "name": "MT-Bench",
  "parent_benchmark_id": null,
  "categories": ["communication", "reasoning", "general", "roleplay"],
  "modality": "text",
  "multilingual": false,
  "max_score": 100.0,
  "language": "en",
  "description": "MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",
  "paper_link": "https://arxiv.org/abs/2306.05685",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.516415+00:00",
  "updated_at": "2025-07-19T19:56:14.516415+00:00"
}