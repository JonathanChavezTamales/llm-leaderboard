{
  "benchmark_id": "mmlu-base",
  "name": "MMLU-Base",
  "parent_benchmark_id": null,
  "categories": ["language", "reasoning", "math", "general"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "Base version of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. Designed to comprehensively measure the breadth and depth of a model's academic and professional understanding.",
  "paper_link": "https://arxiv.org/abs/2009.03300",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.562710+00:00",
  "updated_at": "2025-07-19T19:56:14.562710+00:00"
}