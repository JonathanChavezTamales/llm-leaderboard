{
  "benchmark_id": "openai-mmlu",
  "name": "OpenAI MMLU",
  "parent_benchmark_id": null,
  "categories": ["general", "reasoning", "math", "legal", "healthcare", "finance", "physics", "chemistry", "economics", "psychology"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "MMLU (Massive Multitask Language Understanding) is a comprehensive benchmark that measures a text model's multitask accuracy across 57 diverse academic and professional subjects. The test covers elementary mathematics, US history, computer science, law, morality, business ethics, clinical knowledge, and many other domains spanning STEM, humanities, social sciences, and professional fields. To attain high accuracy, models must possess extensive world knowledge and problem-solving ability.",
  "paper_link": "https://arxiv.org/abs/2009.03300",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.043675+00:00",
  "updated_at": "2025-07-19T19:56:14.043675+00:00"
}