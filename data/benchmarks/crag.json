{
  "benchmark_id": "crag",
  "name": "CRAG",
  "parent_benchmark_id": null,
  "categories": ["reasoning", "search"],
  "modality": "text",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "CRAG (Comprehensive RAG Benchmark) is a factual question answering benchmark consisting of 4,409 question-answer pairs across 5 domains (finance, sports, music, movie, open domain) and 8 question categories. The benchmark includes mock APIs to simulate web and Knowledge Graph search, designed to represent the diverse and dynamic nature of real-world QA tasks with temporal dynamism ranging from years to seconds. It evaluates retrieval-augmented generation systems for trustworthy question answering.",
  "paper_link": "https://arxiv.org/abs/2406.04744",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:12.741280+00:00",
  "updated_at": "2025-07-19T19:56:12.741280+00:00"
}