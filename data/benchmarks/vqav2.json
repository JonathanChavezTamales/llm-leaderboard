{
  "benchmark_id": "vqav2",
  "name": "VQAv2",
  "parent_benchmark_id": null,
  "categories": ["vision", "multimodal", "reasoning"],
  "modality": "multimodal",
  "multilingual": false,
  "max_score": 1.0,
  "language": "en",
  "description": "VQAv2 is a balanced Visual Question Answering dataset that addresses language bias by providing complementary images for each question, forcing models to rely on visual understanding rather than language priors. It contains approximately twice the number of image-question pairs compared to the original VQA dataset.",
  "paper_link": "https://arxiv.org/abs/1612.00837",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.410411+00:00",
  "updated_at": "2025-07-19T19:56:14.410411+00:00"
}