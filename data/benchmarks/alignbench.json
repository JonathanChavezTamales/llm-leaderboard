{
  "benchmark_id": "alignbench",
  "name": "AlignBench",
  "parent_benchmark_id": null,
  "categories": ["general", "language", "math", "reasoning", "roleplay"],
  "modality": "text",
  "multilingual": true,
  "max_score": 1.0,
  "language": "en",
  "description": "AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",
  "paper_link": "https://arxiv.org/abs/2311.18743",
  "implementation_link": null,
  "verified": false,
  "created_at": "2025-07-19T19:56:14.542033+00:00",
  "updated_at": "2025-07-19T19:56:14.542033+00:00"
}