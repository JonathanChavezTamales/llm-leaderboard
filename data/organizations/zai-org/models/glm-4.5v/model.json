{
  "model_id": "glm-4.5v",
  "name": "GLM-4.5V",
  "organization_id": "zai-org",
  "model_family_id": null,
  "fine_tuned_from_model_id": "glm-4.5-air",
  "description": "GLM-4.5V is a multimodal (vision-language) model based on GLM-4.5-Air (106B total, 12B active) that extends hybrid reasoning to images and video. It achieves state-of-the-art results across 40+ VLM benchmarks (image reasoning, video understanding, GUI tasks, chart/document parsing, grounding) while supporting a Thinking Mode switch for deep reasoning. Released under MIT with FP8/BF16 variants and tooling in Transformers, vLLM, and SGLang.",
  "release_date": "2025-08-11",
  "announcement_date": "2025-08-11",
  "license_id": "mit",
  "multimodal": true,
  "knowledge_cutoff": null,
  "param_count": 108000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": null,
  "source_playground": "https://chat.z.ai",
  "source_paper": "https://arxiv.org/abs/2507.01006",
  "source_scorecard_blog_link": null,
  "source_repo_link": "https://github.com/zai-org/GLM-V/",
  "source_weights_link": "https://huggingface.co/zai-org/GLM-4.5V",
  "created_at": "2025-09-29T00:00:00.000000+00:00",
  "updated_at": "2025-09-29T00:00:00.000000+00:00"
}
