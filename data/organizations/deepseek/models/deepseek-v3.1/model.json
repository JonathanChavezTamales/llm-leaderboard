{
  "model_id": "deepseek-v3.1",
  "name": "DeepSeek-V3.1",
  "organization_id": "deepseek",
  "model_family_id": null,
  "fine_tuned_from_model_id": "deepseek-v3",
  "description": "DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",
  "release_date": "2025-01-10",
  "announcement_date": "2025-01-10",
  "license_id": "mit",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 671000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": "https://api.deepseek.com/docs",
  "source_playground": "https://chat.deepseek.com/",
  "source_paper": "https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek-V3.pdf",
  "source_scorecard_blog_link": "https://www.deepseek.com/news/deepseek-v3-1",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-V3",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
  "created_at": "2025-01-10T00:00:00.000000+00:00",
  "updated_at": "2025-09-15T00:00:00.000000+00:00"
}
