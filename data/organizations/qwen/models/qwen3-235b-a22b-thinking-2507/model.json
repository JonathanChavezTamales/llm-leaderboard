{
  "model_id": "qwen3-235b-a22b-thinking-2507",
  "name": "Qwen3-235B-A22B-Thinking-2507",
  "organization_id": "qwen",
  "model_family_id": null,
  "fine_tuned_from_model_id": "qwen3-235b-a22b",
  "description": "Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",
  "release_date": "2025-07-25",
  "announcement_date": "2025-07-25",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 235000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
  "source_playground": "https://chat.qwen.ai/",
  "source_paper": null,
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3-thinking/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
  "created_at": "2025-07-25T00:00:00.000000+00:00",
  "updated_at": "2025-09-15T00:00:00.000000+00:00"
}
