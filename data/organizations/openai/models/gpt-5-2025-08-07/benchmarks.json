[
  {
    "model_benchmark_id": 9002,
    "benchmark_id": "swe-bench-verified",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.749,
    "normalized_score": 0.749,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled (up to 128K tokens) with enhanced reasoning capabilities and iterative problem-solving approach.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "SWE-Bench Verified"
  },
  {
    "model_benchmark_id": 9004,
    "benchmark_id": "aider-polyglot",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.88,
    "normalized_score": 0.88,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled (up to 128K tokens) with step-by-step reasoning and multi-language code understanding.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Aider-Polyglot"
  },
  {
    "model_benchmark_id": 9005,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 1.0,
    "normalized_score": 1.0,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro with thinking mode enabled (python tools) - perfect score on competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9017,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.967,
    "normalized_score": 0.967,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro without thinking mode (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9018,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.996,
    "normalized_score": 0.996,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (python tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9019,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.71,
    "normalized_score": 0.71,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard without thinking mode (python tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9020,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.946,
    "normalized_score": 0.946,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9021,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.619,
    "normalized_score": 0.619,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard without thinking mode (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9009,
    "benchmark_id": "mmmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.842,
    "normalized_score": 0.842,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - College-level visual problem-solving with multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU"
  },
  {
    "model_benchmark_id": 9006,
    "benchmark_id": "mmlu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.925,
    "normalized_score": 0.925,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Standard benchmark across multiple academic subjects with comprehensive knowledge evaluation.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 9007,
    "benchmark_id": "humaneval",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.934,
    "normalized_score": 0.934,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Code generation benchmark with function completion tasks in Python.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 9008,
    "benchmark_id": "math",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.847,
    "normalized_score": 0.847,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled with step-by-step mathematical problem solving and verification.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 9052,
    "benchmark_id": "mmmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.744,
    "normalized_score": 0.744,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - College-level visual problem-solving with multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU"
  },

  {
    "model_benchmark_id": 9013,
    "benchmark_id": "healthbench-hard",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.016,
    "normalized_score": 0.016,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled for medical hallucination detection. Measured inaccuracies on challenging healthcare conversations.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HealthBench Hard"
  },
  {
    "model_benchmark_id": 9014,
    "benchmark_id": "healthbench-hard",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.036,
    "normalized_score": 0.036,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Standard benchmark without thinking mode. Measured inaccuracies on challenging healthcare conversations.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HealthBench Hard"
  },

  {
    "model_benchmark_id": 9022,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.321,
    "normalized_score": 0.321,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro with thinking mode enabled (Python tools) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9023,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.263,
    "normalized_score": 0.263,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (Python tools) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9024,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.135,
    "normalized_score": 0.135,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (no tools) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9025,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.274,
    "normalized_score": 0.274,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "ChatGPT agent with thinking mode (browser + computer + terminal tools) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9026,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 1.0,
    "normalized_score": 1.0,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro with thinking mode enabled (Python tools) - Perfect score on Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  },
  {
    "model_benchmark_id": 9027,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.967,
    "normalized_score": 0.967,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (Python tools) - Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  },
  {
    "model_benchmark_id": 9028,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.933,
    "normalized_score": 0.933,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  },
  {
    "model_benchmark_id": 9029,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.894,
    "normalized_score": 0.894,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 Pro - Diamond thinking with Python tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9030,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.884,
    "normalized_score": 0.884,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 Pro - Diamond thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9031,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.873,
    "normalized_score": 0.873,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Diamond thinking with Python tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9032,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.857,
    "normalized_score": 0.857,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Diamond thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9033,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.778,
    "normalized_score": 0.778,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Diamond no thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9034,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.42,
    "normalized_score": 0.42,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro with thinking mode and search capabilities (Python + search with blocklist) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9035,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.307,
    "normalized_score": 0.307,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 pro with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9036,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.352,
    "normalized_score": 0.352,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode and search capabilities (Python + search with blocklist) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9037,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.248,
    "normalized_score": 0.248,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9038,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.063,
    "normalized_score": 0.063,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard without thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9039,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.416,
    "normalized_score": 0.416,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "ChatGPT agent with thinking mode and full tools (browser + computer + terminal) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9040,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.23,
    "normalized_score": 0.23,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "ChatGPT agent with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9041,
    "benchmark_id": "scale-multichallenge",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.696,
    "normalized_score": 0.696,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Multi-turn instruction following benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Scale MultiChallenge"
  },
  {
    "model_benchmark_id": 9042,
    "benchmark_id": "scale-multichallenge",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.549,
    "normalized_score": 0.549,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Multi-turn instruction following benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Scale MultiChallenge"
  },
  {
    "model_benchmark_id": 9043,
    "benchmark_id": "browsecomp",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.549,
    "normalized_score": 0.549,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Agentic search & browsing benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "BrowseComp"
  },
  {
    "model_benchmark_id": 9044,
    "benchmark_id": "browsecomp",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.689,
    "normalized_score": 0.689,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "ChatGPT agent with thinking mode enabled - Agentic search & browsing benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "BrowseComp"
  },
  {
    "model_benchmark_id": 9045,
    "benchmark_id": "collie",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.99,
    "normalized_score": 0.99,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Instruction-following in freeform writing.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "COLLIE"
  },
  {
    "model_benchmark_id": 9046,
    "benchmark_id": "collie",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.705,
    "normalized_score": 0.705,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Instruction-following in freeform writing.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "COLLIE"
  },
  {
    "model_benchmark_id": 9047,
    "benchmark_id": "tau2-airline",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.728,
    "normalized_score": 0.728,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Function calling benchmark (airline domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 airline"
  },
  {
    "model_benchmark_id": 9048,
    "benchmark_id": "tau2-airline",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.55,
    "normalized_score": 0.55,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Function calling benchmark (airline domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 airline"
  },
  {
    "model_benchmark_id": 9049,
    "benchmark_id": "tau2-retail",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.811,
    "normalized_score": 0.811,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Function calling benchmark (retail domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 retail"
  },
  {
    "model_benchmark_id": 9050,
    "benchmark_id": "tau2-retail",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.581,
    "normalized_score": 0.581,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Function calling benchmark (retail domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 retail"
  },
  {
    "model_benchmark_id": 9051,
    "benchmark_id": "tau2-telecom",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.967,
    "normalized_score": 0.967,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Function calling benchmark (telecom domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 telecom"
  },
  {
    "model_benchmark_id": 9053,
    "benchmark_id": "mmmu-pro",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.784,
    "normalized_score": 0.784,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU-Pro"
  },
  {
    "model_benchmark_id": 9054,
    "benchmark_id": "mmmu-pro",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.627,
    "normalized_score": 0.627,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU-Pro"
  },
  {
    "model_benchmark_id": 9055,
    "benchmark_id": "videommmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.846,
    "normalized_score": 0.846,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Video-based multimodal reasoning (max frame 256).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "VideoMMMU"
  },
  {
    "model_benchmark_id": 9056,
    "benchmark_id": "videommmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.616,
    "normalized_score": 0.616,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Video-based multimodal reasoning (max frame 256).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "VideoMMMU"
  },
  {
    "model_benchmark_id": 9057,
    "benchmark_id": "charxiv-r",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.811,
    "normalized_score": 0.811,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Scientific figure reasoning and interpretation.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "CharXiv-R"
  },
  {
    "model_benchmark_id": 9058,
    "benchmark_id": "charxiv-r",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.578,
    "normalized_score": 0.578,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Scientific figure reasoning and interpretation.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "CharXiv-R"
  },
  {
    "model_benchmark_id": 9059,
    "benchmark_id": "erqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.657,
    "normalized_score": 0.657,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Multimodal spatial reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "ERQA"
  },
  {
    "model_benchmark_id": 9060,
    "benchmark_id": "erqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.42,
    "normalized_score": 0.42,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 without thinking mode - Multimodal spatial reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "ERQA"
  }
]
