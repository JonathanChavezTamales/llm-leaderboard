{
  "model_id": "gpt-oss-120b",
  "name": "GPT OSS 120B",
  "organization_id": "openai",
  "fine_tuned_from_model_id": null,
  "description": "GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",
  "release_date": "2025-08-05",
  "announcement_date": "2025-08-05",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 116800000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": null,
  "source_playground": "https://gpt-oss.com/",
  "source_paper": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
  "source_scorecard_blog_link": "https://openai.com/index/gpt-oss-model-card/",
  "source_repo_link": "https://github.com/openai/gpt-oss",
  "source_weights_link": "https://huggingface.co/openai/gpt-oss-120b",
  "created_at": "2025-08-05T19:49:05.852855+00:00",
  "updated_at": "2025-08-05T19:49:05.852855+00:00",
  "model_family_id": null
}
